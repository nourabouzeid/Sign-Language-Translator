{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be24b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e60cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(static_image_mode=True, model_complexity=1, smooth_landmarks=True)\n",
    "\n",
    "# Initialize MediaPipe Drawing\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "drawing_spec = mp_drawing.DrawingSpec(color=(128,0,128),thickness=1, circle_radius=1)\n",
    "connect_drawing_spec = mp_drawing.DrawingSpec(color=(200,200,200),thickness=1, circle_radius=1)\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    results = holistic.process(image)\n",
    "    image_copy=image.copy()\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_copy, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=drawing_spec, connection_drawing_spec=drawing_spec)\n",
    "    if results.face_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_copy, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=drawing_spec, connection_drawing_spec=connect_drawing_spec)\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_copy, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=drawing_spec, connection_drawing_spec=drawing_spec)\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_copy, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=drawing_spec, connection_drawing_spec=drawing_spec)\n",
    "    return image_copy, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dc22d25-eca8-4d33-9fd2-c9be2ab6616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Load the model without compiling it\n",
    "model = load_model(r'C:\\Users\\kabou\\Desktop\\Nour\\Courses\\HCIB - AI\\Project\\9_Sign_Model.h5', compile=False)\n",
    "\n",
    "# Step 2: Recompile the model with the Adam optimizer\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e21de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to preprocess input\n",
    "def preprocess_input(frames):\n",
    "    # Assuming frames is a list of image frames\n",
    "    # Here you would extract landmarks or any other features needed for the model\n",
    "    images, landmarks = extract_landmarks(frames)\n",
    "\n",
    "    # Assuming the output of extract_landmarks is (num_frames, 21, 6)\n",
    "    num_frames = len(landmarks)\n",
    "    max_frames = 25  # The length used during training\n",
    "\n",
    "    # If fewer frames, pad with zeros; if more, truncate\n",
    "    if num_frames < max_frames:\n",
    "        padding_length = max_frames - num_frames\n",
    "        landmarks = np.pad(landmarks, ((0, padding_length), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
    "    elif num_frames > max_frames:\n",
    "        landmarks = landmarks[:max_frames]\n",
    "\n",
    "    landmarks = landmarks.reshape(1, max_frames, -1)\n",
    "    return images,landmarks\n",
    "\n",
    "# Function to extract landmarks from frames (example placeholder function)\n",
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10b520f9-4630-43bc-a59d-d4a38a41fed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "sequence = []\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-25:]\n",
    "        \n",
    "        if len(sequence) == 25:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0),verbose=0)[0]\n",
    "            predicted_class = np.argmax(res)\n",
    "            predicted_class_value = res[predicted_class]\n",
    "            if predicted_class_value>0.7:\n",
    "                print(f\"Predicted class: {predicted_class}\")\n",
    "                print(f\"Predicted Class Value: {predicted_class_value}\")\n",
    "\n",
    "    # Display the frame\n",
    "        cv2.imshow('Frame', image)\n",
    "    \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cc2cd-7729-4db7-888d-28b59b4e0ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915fb46-a4b6-431c-adad-e908bc42ab5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
